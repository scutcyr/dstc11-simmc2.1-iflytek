#!/usr/bin/env python3
"""
Copyright (c) Facebook, Inc. and its affiliates.
All rights reserved.
This source code is licensed under the license found in the LICENSE file in the
root directory of this source tree.


    Scripts for evaluating the GPT-2 DST model predictions.

    First, we parse the line-by-line stringified format into
    the structured DST output.

    We then run the main DST Evaluation script to get results.

Updated by Yirong Chen 
Check for SIMMC 2.1
Mail: [yrchen5@iflytek.com](mailto:yrchen5@iflytek.com) or [eeyirongchen@mail.scut.edu.cn](mailto:eeyirongchen@mail.scut.edu.cn)
Date: 2022/08/18

说明：同时评测DSTC-11 SIMMC2.1的4个子任务并保存结果


Usage:
cd <project_path>/dstc10_team4_bart
python ./evaluation_tools/evaluate_all_task.py \
  --input_path_target=../data_object_special_with_disambiguation_candidates/simmc2.1_dials_dstc11_devtest_target_with_disambiguation_candidates.txt \
  --input_path_predicted=./results/devtest_results/dstc11-simmc2.1-devtest-pred-subtask-1-to-4-of-mt-bart-large_with_disam_candi_ky_20220817_1600_check_isnocoref_and_coref.txt \
  --output_path_report=./results/devtest_results/dstc11-simmc2.1-devtest-pred-subtask-1-to-4-of-mt-bart-large_with_disam_candi_ky_20220817_1600_check_isnocoref_and_coref_report_of_subtask1_to_4.json


"""
import nltk
import json
import argparse
import numpy as np
from convert import parse_flattened_results_from_file
from evaluate_dst import evaluate_from_flat_list
from response_evaluation import evaluate_response_generation

def normalize_sentence(sentence):
    """Normalize the sentences and tokenize."""
    return nltk.tokenize.word_tokenize(sentence.lower())


def parse_response_from_file(input_path):
    """Parses the response from a flattened file.

    Args:
        input_path: Path to read the responses from.
    """
    lines = []
    with open(input_path, "r") as file_id:
        for ii in file_id.readlines():
            split_line = ii.split("<EOB>", 1)
            # print(split_line)
            # 增加该判断，因为部分样例没有生成回复，没有"<EOB>"标记
            # Updated by Yirong Chen
            if len(split_line)==1:
                lines.append(
                    (split_line[0].strip("\n"), "Sorry, I don't understand what you mean.")
                )
            else:
                lines.append(
                    (split_line[0].strip("\n"), split_line[1].strip("\n").strip("<EOS>"))
                )
    return lines

if __name__ == "__main__":
    # Parse input args
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--input_path_target", help="path for target, line-separated format (.txt)"
    )
    parser.add_argument(
        "--input_path_predicted",
        help="path for model prediction output, line-separated format (.txt)",
    )
    parser.add_argument(
        "--data_json_path",
        default="../data/simmc2_dials_dstc10_devtest.json",
        help="Data with .json format gold responses",
    )
    parser.add_argument(
        "--output_json_response_path", default=None, help="Responses generated by the model"
    )
    parser.add_argument(
        "--output_path_report", help="path for saving evaluation summary (.json)"
    )
    parser.add_argument(
        "--dialog_meta_data",
        type=str,
        default='../data_object_special/simmc2_dials_dstc10_devtest_inference_disambiguation.json'
    )
    parser.add_argument(
        "--single_round_evaluation",
        dest="single_round_evaluation",
        action="store_true",
        default=False,
        help="Single round evaluation for hidden split",
    )

    args = parser.parse_args()
    input_path_target = args.input_path_target
    input_path_predicted = args.input_path_predicted
    output_path_report = args.output_path_report

    # Convert the data from the GPT-2 friendly format to JSON
    list_target = parse_flattened_results_from_file(input_path_target)
    list_predicted = parse_flattened_results_from_file(input_path_predicted)

    # Evaluate Subtask 1 ~ Subtask 3
    report = evaluate_from_flat_list(list_target, list_predicted)
    print(report)

    # Evaluate Subtask 4
    if args.single_round_evaluation:

        with open(args.data_json_path, "r") as file_id:
            gt_responses = json.load(file_id)

        #print(gt_responses)
        #print(gt_responses[0])

        dialog_meta_data = json.load(open(args.dialog_meta_data)) # List[Dict]

        predicted_response = [] 

        with open(input_path_predicted, 'r') as f:
            lines = f.readlines()
            assert len(lines) == len(dialog_meta_data)
            for line, meta in zip(lines, dialog_meta_data):
                response = line.split("<EOB>")[1].split("<EOS>")[0].strip()
                predicted_response.append({
                    "dialog_id" : meta["dialog_id"],
                    "predictions" : [{
                        "turn_id" : meta["turn_id"],
                        "response" : response
                    }]
                })
        if args.output_json_response_path:
            json.dump(predicted_response, open(args.output_json_response_path, "w"), indent=4)


        bleu_score, bleu_std_err = evaluate_response_generation(
            gt_responses, predicted_response, args.single_round_evaluation
        )
        print(f"BLEU Score: {bleu_score} +- {bleu_std_err}")

        report["bleu"] = bleu_score
        report["bleu_stderr"] = bleu_std_err
    else:
        # Convert the data from the model friendly format to JSON
        list_target = parse_response_from_file(input_path_target)
        list_predicted = parse_response_from_file(input_path_predicted)
        # Compute BLEU scores.
        bleu_scores = []
        # Smoothing function.
        chencherry = nltk.translate.bleu_score.SmoothingFunction()

        for response, gt_response in zip(list_predicted, list_target):
            #print("预测回复：", response[0])
            #print("真实回复：", gt_response[0])
            #assert response[0] == gt_response[0], "Input contexts do not match!"
            bleu_score = nltk.translate.bleu_score.sentence_bleu(
                [normalize_sentence(gt_response[1])],
                normalize_sentence(response[1]),
                smoothing_function=chencherry.method7,
            )
            bleu_scores.append(bleu_score)
        mean_bleu_scores = np.mean(bleu_scores)
        mean_bleu_scores_std = np.std(bleu_scores) / np.sqrt(len(bleu_scores))

        report["bleu"] = mean_bleu_scores
        report["bleu_stderr"] = mean_bleu_scores_std

        print(
            "BLEU score: {} +- {}".format(
                mean_bleu_scores, mean_bleu_scores_std
            )
        )

    # Save report
    with open(output_path_report, "w") as f_out:
        json.dump(report, f_out)
